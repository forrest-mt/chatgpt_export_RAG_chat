{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348ad3e5-8935-4518-bf55-ffd72bbfb64b",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8206fc73-bfec-4800-92ad-81e56295dba9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import chromadb\n",
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210245c9-0f21-46a6-9ba9-1585a6246d93",
   "metadata": {},
   "source": [
    "## load the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8887a0c4-e42e-4f81-8afa-d049e2b7c093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 437 conversations\n"
     ]
    }
   ],
   "source": [
    "# Load the conversations data first\n",
    "import json\n",
    "\n",
    "json_file_path = \"/Users/michaeltrang/Documents/Coding Projects/AI Builders Bootcamp/hw3_chatgpt_history_chat/chatgpt_data/conversations.json\"\n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    conversations_data = json.load(f)\n",
    "\n",
    "print(f\"✅ Loaded {len(conversations_data)} conversations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b0aa7-7974-4979-8291-6eae6e9c2f41",
   "metadata": {},
   "source": [
    "## JSON extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfca1d93-0089-41f1-af47-27c650b06d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed message extraction function\n",
    "def get_conversation_messages(conversation):\n",
    "    \"\"\"Extract messages from a conversation using the correct ChatGPT structure\"\"\"\n",
    "    messages = []\n",
    "    mapping = conversation.get(\"mapping\", {})\n",
    "    \n",
    "    # Get all nodes that have messages\n",
    "    for node_id, node_data in mapping.items():\n",
    "        message = node_data.get(\"message\")\n",
    "        if not message:\n",
    "            continue\n",
    "            \n",
    "        # Get the content\n",
    "        content = message.get(\"content\")\n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        # Check if it's text content\n",
    "        if content.get(\"content_type\") == \"text\":\n",
    "            parts = content.get(\"parts\", [])\n",
    "            if parts and len(parts) > 0:\n",
    "                # Get the author role\n",
    "                author_role = message.get(\"author\", {}).get(\"role\", \"\")\n",
    "                \n",
    "                # Skip system messages unless they're user system messages\n",
    "                if author_role == \"system\":\n",
    "                    metadata = message.get(\"metadata\", {})\n",
    "                    if not metadata.get(\"is_user_system_message\", False):\n",
    "                        continue\n",
    "                \n",
    "                # Clean up author names\n",
    "                if author_role == \"assistant\":\n",
    "                    author = \"ChatGPT\"\n",
    "                elif author_role == \"user\":\n",
    "                    author = \"Michael\"\n",
    "                elif author_role == \"system\":\n",
    "                    author = \"System\"\n",
    "                else:\n",
    "                    author = author_role\n",
    "                \n",
    "                # Add the message\n",
    "                messages.append({\n",
    "                    \"author\": author,\n",
    "                    \"text\": parts[0],\n",
    "                    \"timestamp\": message.get(\"create_time\"),\n",
    "                    \"message_id\": message.get(\"id\"),\n",
    "                    \"node_id\": node_id\n",
    "                })\n",
    "    \n",
    "    # Sort by timestamp to get chronological order\n",
    "    messages.sort(key=lambda x: x[\"timestamp\"] if x[\"timestamp\"] else 0)\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ac7d0-628f-4140-84f8-3197073b9746",
   "metadata": {},
   "source": [
    "## Test conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27eebc16-195f-4be9-8e0a-35beaa38ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing message extraction on first conversation...\n",
      "--------------------------------------------------\n",
      "Conversation: Montessori for self-growth\n",
      "Found 21 messages\n",
      "\n",
      "First few messages:\n",
      "\n",
      "1. Michael (2025-09-03 14:34:30):\n",
      "   how do I incorporate concepts of ideas of Montessori method into my own development and growth...\n",
      "\n",
      "2. ChatGPT (2025-09-03 14:34:31):\n",
      "   ...\n",
      "\n",
      "3. ChatGPT (2025-09-03 14:34:31):\n",
      "   That’s a really thoughtful question, Michael. Montessori is usually associated with children, but its principles can be surprisingly powerful for adul...\n",
      "\n",
      "4. Michael (2025-09-03 14:37:17):\n",
      "   can you go deeper here?Self-Directed Learning → Self-Agency in Growth\n",
      "\n",
      "Montessori idea: Children choose work, set their pace, and self-correct.\n",
      "\n",
      "Adult...\n",
      "\n",
      "5. ChatGPT (2025-09-03 14:37:18):\n",
      "   Got it — let’s really zoom in on **Self-Directed Learning → Self-Agency in Growth**, because this is where Montessori becomes especially powerful for ...\n"
     ]
    }
   ],
   "source": [
    "# Test the extraction on the first conversation\n",
    "if conversations_data:\n",
    "    print(\"Testing message extraction on first conversation...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    first_conv = conversations_data[0]\n",
    "    title = first_conv.get('title', 'No title')\n",
    "    print(f\"Conversation: {title}\")\n",
    "    \n",
    "    messages = get_conversation_messages(first_conv)\n",
    "    print(f\"Found {len(messages)} messages\")\n",
    "    \n",
    "    # Show first few messages\n",
    "    print(\"\\nFirst few messages:\")\n",
    "    for i, msg in enumerate(messages[:5]):\n",
    "        timestamp = msg['timestamp']\n",
    "        if timestamp:\n",
    "            from datetime import datetime\n",
    "            dt = datetime.fromtimestamp(timestamp)\n",
    "            time_str = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            time_str = \"No timestamp\"\n",
    "            \n",
    "        print(f\"\\n{i+1}. {msg['author']} ({time_str}):\")\n",
    "        print(f\"   {msg['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad4fc3-f5e9-4c84-94fa-5291fc3d5811",
   "metadata": {},
   "source": [
    "## process and chunk the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d6ff243-bc59-4c82-9515-3d6cb337015f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all conversations...\n",
      "--------------------------------------------------\n",
      "Processing 1/437: 'Montessori for self-growth' - 21 messages\n",
      "Processing 2/437: 'Ilama Index explanation' - 41 messages\n",
      "Processing 3/437: 'Okonomiyaki cooking styles' - 3 messages\n",
      "...\n",
      "Processing 435/437: 'Test Assistance Available' - 2 messages\n",
      "Processing 436/437: 'New chat' - 1 messages\n",
      "Processing 437/437: 'Social Hobbies for Gamers' - 11 messages\n",
      "\n",
      "✅ Total messages extracted: 4424\n",
      "\n",
      "Message breakdown:\n",
      "  Michael: 1841 messages\n",
      "  ChatGPT: 2445 messages\n",
      "  tool: 138 messages\n",
      "\n",
      "Sample messages by author:\n",
      "\n",
      "Michael:\n",
      "  how do I incorporate concepts of ideas of Montessori method into my own development and growth...\n",
      "\n",
      "ChatGPT:\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "# Process all conversations and create chunks\n",
    "print(\"Processing all conversations...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_messages = []\n",
    "total_conversations = len(conversations_data)\n",
    "\n",
    "for conv_idx, conversation in enumerate(conversations_data):\n",
    "  title = conversation.get('title', f'Conversation {conv_idx}')\n",
    "  messages = get_conversation_messages(conversation)\n",
    "\n",
    "  # Only print for first 3 and last 3 conversations\n",
    "  if conv_idx < 3 or conv_idx >= total_conversations - 3:\n",
    "    print(f\"Processing {conv_idx + 1}/{total_conversations}: '{title}' - {len(messages)} messages\")\n",
    "  elif conv_idx == 3:\n",
    "      print(\"...\")  # Show ellipsis after first 3\n",
    "\n",
    "  for msg_idx, msg in enumerate(messages):\n",
    "      message_info = {\n",
    "          'conversation_id': conv_idx,\n",
    "          'conversation_title': title,\n",
    "          'message_id': f\"{conv_idx}_{msg_idx}\",\n",
    "          'author': msg['author'],\n",
    "          'content': msg['text'],\n",
    "          'timestamp': msg['timestamp'],\n",
    "          'source': 'ChatGPT',\n",
    "          'chunk_type': 'message'\n",
    "      }\n",
    "      all_messages.append(message_info)\n",
    "\n",
    "print(f\"\\n✅ Total messages extracted: {len(all_messages)}\")\n",
    "\n",
    "# Show some statistics\n",
    "authors = [msg['author'] for msg in all_messages]\n",
    "from collections import Counter\n",
    "author_counts = Counter(authors)\n",
    "print(f\"\\nMessage breakdown:\")\n",
    "for author, count in author_counts.items():\n",
    "  print(f\"  {author}: {count} messages\")\n",
    "\n",
    "# Show sample of different types of messages\n",
    "print(f\"\\nSample messages by author:\")\n",
    "for author in ['Michael', 'ChatGPT']:\n",
    "  sample_msg = next((msg for msg in all_messages if msg['author'] == author), None)\n",
    "  if sample_msg:\n",
    "      print(f\"\\n{author}:\")\n",
    "      print(f\"  {sample_msg['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55025a53-30e7-4487-bb9a-4bc904c4aea2",
   "metadata": {},
   "source": [
    "## Test script for conversations with metadata to see if they match spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbcf0669-06ba-4f50-b397-82002019a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conversation 2...\n",
      "--------------------------------------------------\n",
      "Conversation: Okonomiyaki cooking styles\n",
      "Found 3 messages\n",
      "\n",
      "First few messages with metadata:\n",
      "\n",
      "--- Message 1 ---\n",
      "Author: Michael (2025-09-04 09:14:40)\n",
      "Content: In Japan, do they serve okonomiyaki mostly precooked? I went to a spot and they gave me a mostly coo...\n",
      "Content length: 162 chars, 32 words\n",
      "\n",
      "Metadata:\n",
      "  chunk_id: ffad621a-0a25-4db5-8278-b9ff8c4cdf8e\n",
      "  chunk_type: michael\n",
      "  creation_date: 2025-09-04T09:14:40.441990\n",
      "  source_id: chatgpt_conv_2\n",
      "  source_name: ChatGPT - Okonomiyaki cooking styles\n",
      "  tags: []\n",
      "  token_estimate: 41\n",
      "  author: Michael\n",
      "\n",
      "--- Message 2 ---\n",
      "Author: ChatGPT (2025-09-04 09:14:40)\n",
      "Content: ...\n",
      "Content length: 0 chars, 0 words\n",
      "\n",
      "Metadata:\n",
      "  chunk_id: eb5690b8-f711-4bca-95d3-79edae02f200\n",
      "  chunk_type: chatgpt\n",
      "  creation_date: 2025-09-04T09:14:40.686570\n",
      "  source_id: chatgpt_conv_2\n",
      "  source_name: ChatGPT - Okonomiyaki cooking styles\n",
      "  tags: []\n",
      "  token_estimate: 0\n",
      "  author: ChatGPT\n",
      "\n",
      "--- Message 3 ---\n",
      "Author: ChatGPT (2025-09-04 09:14:40)\n",
      "Content: What you experienced is actually pretty common in Japan—it depends on the style of the okonomiyaki r...\n",
      "Content length: 1643 chars, 269 words\n",
      "\n",
      "Metadata:\n",
      "  chunk_id: c284119d-4238-4486-9330-793597599b6b\n",
      "  chunk_type: chatgpt\n",
      "  creation_date: 2025-09-04T09:14:40.697702\n",
      "  source_id: chatgpt_conv_2\n",
      "  source_name: ChatGPT - Okonomiyaki cooking styles\n",
      "  tags: []\n",
      "  token_estimate: 349\n",
      "  author: ChatGPT\n",
      "\n",
      "Message breakdown:\n",
      "  Michael: 1 messages\n",
      "  ChatGPT: 2 messages\n"
     ]
    }
   ],
   "source": [
    "# Test a different conversation with metadata\n",
    "conversation_index = 2  # Change this number to test different conversations (0-436)\n",
    "\n",
    "if conversations_data and conversation_index < len(conversations_data):\n",
    "    print(f\"Testing conversation {conversation_index}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    conv = conversations_data[conversation_index]\n",
    "    title = conv.get('title', 'No title')\n",
    "    print(f\"Conversation: {title}\")\n",
    "    \n",
    "    messages = get_conversation_messages(conv)\n",
    "    print(f\"Found {len(messages)} messages\")\n",
    "    \n",
    "    # Show first few messages with metadata\n",
    "    print(\"\\nFirst few messages with metadata:\")\n",
    "    for i, msg in enumerate(messages[:3]):  # Show first 3 for brevity\n",
    "        timestamp = msg['timestamp']\n",
    "        if timestamp:\n",
    "            from datetime import datetime\n",
    "            dt = datetime.fromtimestamp(timestamp)\n",
    "            time_str = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            time_str = \"No timestamp\"\n",
    "        \n",
    "        # Create metadata according to PRD schema\n",
    "        creation_date = \"Unknown\"\n",
    "        if msg['timestamp']:\n",
    "            dt = datetime.fromtimestamp(msg['timestamp'])\n",
    "            creation_date = dt.isoformat()\n",
    "        \n",
    "        metadata = {\n",
    "            'chunk_id': msg['message_id'],\n",
    "            'chunk_type': msg['author'].lower(),\n",
    "            'creation_date': creation_date,\n",
    "            'source_id': f\"chatgpt_conv_{conversation_index}\",\n",
    "            'source_name': f\"ChatGPT - {title}\",\n",
    "            'tags': [],\n",
    "            'token_estimate': int(len(msg['text'].split()) * 1.3),\n",
    "            'author': msg['author']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n--- Message {i+1} ---\")\n",
    "        print(f\"Author: {msg['author']} ({time_str})\")\n",
    "        print(f\"Content: {msg['text'][:100]}...\")\n",
    "        print(f\"Content length: {len(msg['text'])} chars, {len(msg['text'].split())} words\")\n",
    "        \n",
    "        print(f\"\\nMetadata:\")\n",
    "        for key, value in metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "    # Show conversation stats\n",
    "    authors = [msg['author'] for msg in messages]\n",
    "    from collections import Counter\n",
    "    author_counts = Counter(authors)\n",
    "    print(f\"\\nMessage breakdown:\")\n",
    "    for author, count in author_counts.items():\n",
    "        print(f\"  {author}: {count} messages\")\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ Conversation {conversation_index} not found. Available: 0-{len(conversations_data)-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6740e-fb15-4976-a22c-b5955248fcc8",
   "metadata": {},
   "source": [
    "## create embeddings and store in ChromaDB locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df57021b-8b7a-47f6-bb3f-995afe2ef5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up embeddings and ChromaDB...\n",
      "✅ Embedding model loaded\n",
      "✅ ChromaDB ready\n",
      "Creating embeddings for 4424 messages...\n",
      "Batch 1 (100 messages)...\n",
      "Batch 2 (100 messages)...\n",
      "Batch 3 (100 messages)...\n",
      "Batch 4 (100 messages)...\n",
      "Batch 5 (100 messages)...\n",
      "Batch 6 (100 messages)...\n",
      "Batch 7 (100 messages)...\n",
      "Batch 8 (100 messages)...\n",
      "Batch 9 (100 messages)...\n",
      "Batch 10 (100 messages)...\n",
      "Batch 11 (100 messages)...\n",
      "Batch 12 (100 messages)...\n",
      "Batch 13 (100 messages)...\n",
      "Batch 14 (100 messages)...\n",
      "Batch 15 (100 messages)...\n",
      "Batch 16 (100 messages)...\n",
      "Batch 17 (100 messages)...\n",
      "Batch 18 (100 messages)...\n",
      "Batch 19 (100 messages)...\n",
      "Batch 20 (100 messages)...\n",
      "Batch 21 (100 messages)...\n",
      "Batch 22 (100 messages)...\n",
      "Batch 23 (100 messages)...\n",
      "Batch 24 (100 messages)...\n",
      "Batch 25 (100 messages)...\n",
      "Batch 26 (100 messages)...\n",
      "Batch 27 (100 messages)...\n",
      "Batch 28 (100 messages)...\n",
      "Batch 29 (100 messages)...\n",
      "Batch 30 (100 messages)...\n",
      "Batch 31 (100 messages)...\n",
      "Batch 32 (100 messages)...\n",
      "Batch 33 (100 messages)...\n",
      "Batch 34 (100 messages)...\n",
      "Batch 35 (100 messages)...\n",
      "Batch 36 (100 messages)...\n",
      "Batch 37 (100 messages)...\n",
      "Batch 38 (100 messages)...\n",
      "Batch 39 (100 messages)...\n",
      "Batch 40 (100 messages)...\n",
      "Batch 41 (100 messages)...\n",
      "Batch 42 (100 messages)...\n",
      "Batch 43 (100 messages)...\n",
      "Batch 44 (100 messages)...\n",
      "Batch 45 (24 messages)...\n",
      "✅ Stored 4424 messages in ChromaDB!\n",
      "\n",
      "Test search results:\n",
      "1. Michael: how do I incorporate concepts of ideas of Montessori method into my own development and growth...\n",
      "2. ChatGPT: Got it — let’s really zoom in on **Self-Directed Learning → Self-Agency in Growth**, because this is...\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings and store in ChromaDB\n",
    "\n",
    "print(\"Setting up embeddings and ChromaDB...\")\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✅ Embedding model loaded\")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_or_create_collection(name=\"chat_history\")\n",
    "print(\"✅ ChromaDB ready\")\n",
    "\n",
    "# Process messages in batches\n",
    "print(f\"Creating embeddings for {len(all_messages)} messages...\")\n",
    "batch_size = 100\n",
    "\n",
    "for batch_idx in range(0, len(all_messages), batch_size):\n",
    "    batch = all_messages[batch_idx:batch_idx + batch_size]\n",
    "    batch_num = (batch_idx // batch_size) + 1\n",
    "    \n",
    "    print(f\"Batch {batch_num} ({len(batch)} messages)...\")\n",
    "    \n",
    "    texts = []\n",
    "    ids = []\n",
    "    metadatas = []\n",
    "    \n",
    "    for msg in batch:\n",
    "        # Create metadata according to schema\n",
    "        creation_date = \"Unknown\"\n",
    "        if msg['timestamp']:\n",
    "            dt = datetime.fromtimestamp(msg['timestamp'])\n",
    "            creation_date = dt.isoformat()\n",
    "        \n",
    "        metadata = {\n",
    "            'chunk_id': msg['message_id'],\n",
    "            'chunk_type': msg['author'].lower(),\n",
    "            'creation_date': creation_date,\n",
    "            'source_id': f\"chatgpt_conv_{msg['conversation_id']}\",\n",
    "            'source_name': f\"ChatGPT - {msg['conversation_title']}\",\n",
    "            'tags': \"\",\n",
    "            'token_estimate': int(len(msg['content'].split()) * 1.3),\n",
    "            'author': msg['author']\n",
    "        }\n",
    "        \n",
    "        texts.append(msg['content'])\n",
    "        ids.append(msg['message_id'])\n",
    "        metadatas.append(metadata)\n",
    "    \n",
    "    # Create embeddings and store\n",
    "    embeddings = embedding_model.encode(texts).tolist()\n",
    "    collection.add(embeddings=embeddings, documents=texts, metadatas=metadatas, ids=ids)\n",
    "\n",
    "print(f\"✅ Stored {len(all_messages)} messages in ChromaDB!\")\n",
    "\n",
    "# Quick test\n",
    "test_results = collection.query(query_texts=[\"Montessori method\"], n_results=2)\n",
    "print(\"\\nTest search results:\")\n",
    "for i, (doc, metadata) in enumerate(zip(test_results['documents'][0], test_results['metadatas'][0])):\n",
    "    print(f\"{i+1}. {metadata['author']}: {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61059024-2201-44f5-b735-bec9a6e08ec2",
   "metadata": {},
   "source": [
    "## Test ChromaDB storage and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac64c474-90d2-4927-8eb5-30b655c1407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ChromaDB storage...\n",
      "----------------------------------------\n",
      "Collection name: chat_history\n",
      "Collection count: 4424\n",
      "\n",
      "Testing semantic search with 3 queries:\n",
      "\n",
      "--- Query: 'Montessori method' ---\n",
      "Found 3 results:\n",
      "  1. Michael (michael)\n",
      "     Source: ChatGPT - Montessori for self-growth\n",
      "     Content: how do I incorporate concepts of ideas of Montessori method into my own developm...\n",
      "  2. ChatGPT (chatgpt)\n",
      "     Source: ChatGPT - Montessori for self-growth\n",
      "     Content: Got it — let’s really zoom in on **Self-Directed Learning → Self-Agency in Growt...\n",
      "  3. Michael (michael)\n",
      "     Source: ChatGPT - Montessori for self-growth\n",
      "     Content: can you go deeper here?Self-Directed Learning → Self-Agency in Growth\n",
      "\n",
      "Montessor...\n",
      "\n",
      "--- Query: 'coding and programming' ---\n",
      "Found 3 results:\n",
      "  1. Michael (michael)\n",
      "     Source: ChatGPT - Virtual environment in coding\n",
      "     Content: what is a virtual environment in the context of coding...\n",
      "  2. Michael (michael)\n",
      "     Source: ChatGPT - Happy 1 Year!\n",
      "     Content: what are the key principles of computational thinking...\n",
      "  3. ChatGPT (chatgpt)\n",
      "     Source: ChatGPT - Happy 1 Year!\n",
      "     Content: Certainly! Let's explore examples for each of the key principles of computationa...\n",
      "\n",
      "--- Query: 'yoga and health' ---\n",
      "Found 3 results:\n",
      "  1. Michael (michael)\n",
      "     Source: ChatGPT - Yoga after gum graft\n",
      "     Content: When can i start doing yoga or playing Padel ...\n",
      "  2. ChatGPT (chatgpt)\n",
      "     Source: ChatGPT - New chat\n",
      "     Content: For managing stress and enhancing relaxation, some less obvious tools and method...\n",
      "  3. Michael (michael)\n",
      "     Source: ChatGPT - Peter Attia's \"Outlive\" Summary\n",
      "     Content: Outlive is a book that compiles the latest science on health and longevity, comb...\n",
      "\n",
      "--- Testing specific conversation search ---\n",
      "Found 5 results from conversation 0:\n",
      "  1. Michael: how do I incorporate concepts of ideas of Montessori method ...\n",
      "  2. ChatGPT: Got it — let’s really zoom in on **Self-Directed Learning → ...\n",
      "  3. ChatGPT: That’s a really thoughtful question, Michael. Montessori is ...\n",
      "  4. Michael: can you go deeper here?Self-Directed Learning → Self-Agency ...\n",
      "  5. ChatGPT: Yes — if we zoom out from Montessori and look at **personal ...\n",
      "\n",
      "--- Checking metadata structure ---\n",
      "Sample metadata fields:\n",
      "  source_name: ChatGPT - Test Assistance Available (str)\n",
      "  tags:  (str)\n",
      "  creation_date: 2023-05-10T15:11:50.566374 (str)\n",
      "  source_id: chatgpt_conv_434 (str)\n",
      "  chunk_id: 434_0 (str)\n",
      "  chunk_type: michael (str)\n",
      "  author: Michael (str)\n",
      "  token_estimate: 1 (int)\n",
      "\n",
      "✅ ChromaDB test complete!\n"
     ]
    }
   ],
   "source": [
    "# Test ChromaDB storage and embeddings\n",
    "print(\"Testing ChromaDB storage...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 1. Check collection info\n",
    "print(f\"Collection name: {collection.name}\")\n",
    "print(f\"Collection count: {collection.count()}\")\n",
    "\n",
    "# 2. Test semantic search with different queries\n",
    "test_queries = [\n",
    "    \"Montessori method\",\n",
    "    \"coding and programming\", \n",
    "    \"yoga and health\",\n",
    "]\n",
    "\n",
    "print(f\"\\nTesting semantic search with {len(test_queries)} queries:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\n--- Query: '{query}' ---\")\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=3\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(results['documents'][0])} results:\")\n",
    "    for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "        print(f\"  {i+1}. {metadata['author']} ({metadata['chunk_type']})\")\n",
    "        print(f\"     Source: {metadata['source_name']}\")\n",
    "        print(f\"     Content: {doc[:80]}...\")\n",
    "\n",
    "# 3. Test with a specific conversation\n",
    "print(f\"\\n--- Testing specific conversation search ---\")\n",
    "results = collection.query(\n",
    "    query_texts=[\"Montessori\"],\n",
    "    n_results=5,\n",
    "    where={\"source_id\": \"chatgpt_conv_0\"}  # First conversation\n",
    ")\n",
    "print(f\"Found {len(results['documents'][0])} results from conversation 0:\")\n",
    "for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "    print(f\"  {i+1}. {metadata['author']}: {doc[:60]}...\")\n",
    "\n",
    "# 4. Check metadata fields\n",
    "print(f\"\\n--- Checking metadata structure ---\")\n",
    "sample_result = collection.query(query_texts=[\"test\"], n_results=1)\n",
    "if sample_result['metadatas'][0]:\n",
    "    sample_metadata = sample_result['metadatas'][0][0]\n",
    "    print(\"Sample metadata fields:\")\n",
    "    for key, value in sample_metadata.items():\n",
    "        print(f\"  {key}: {value} ({type(value).__name__})\")\n",
    "\n",
    "print(f\"\\n✅ ChromaDB test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879253b8-8d55-42ba-ba3c-c088cb2b4972",
   "metadata": {},
   "source": [
    "Integrate LlamaIndex and setup Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3856acc-5436-4c53-8609-97b2917e6220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LlamaIndex with existing ChromaDB...\n",
      "✅ LlamaIndex settings configured\n",
      "✅ Ollama connection working!\n",
      "Test response: Okay, sounds good! How can I help you with your test? Do you have any questions for me, or would you...\n"
     ]
    }
   ],
   "source": [
    "## LlamaIndex Integration with ChromaDB\n",
    "\n",
    "print(\"Setting up LlamaIndex with existing ChromaDB...\")\n",
    "\n",
    "# Import LlamaIndex components\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# Set up the embedding model (same as before)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Set up Ollama LLM\n",
    "Settings.llm = Ollama(model=\"gemma3:1b\", request_timeout=60.0)\n",
    "\n",
    "# Higher quality synthesis\n",
    "# Settings.llm = Ollama(model=\"gemma3:4b\", request_timeout=90.0)\n",
    "\n",
    "print(\"✅ LlamaIndex settings configured\")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    # Test a simple query\n",
    "    test_response = Settings.llm.complete(\"Hello, this is a test.\")\n",
    "    print(f\"✅ Ollama connection working!\")\n",
    "    print(f\"Test response: {test_response.text[:100]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Still having issues: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ebf15c1-a0bd-40e0-953c-ed2d45ba51a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LlamaIndex index created from existing ChromaDB\n",
      "Index ready for queries\n"
     ]
    }
   ],
   "source": [
    "## Create LlamaIndex Vector Store from existing ChromaDB\n",
    "\n",
    "# Create ChromaVectorStore from existing collection\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "# Create storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Create the index\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    storage_context=storage_context\n",
    ")\n",
    "\n",
    "print(\"✅ LlamaIndex index created from existing ChromaDB\")\n",
    "print(f\"Index ready for queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a033b458-6398-41a7-b52e-6df008df59ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG Query Engine created\n",
      "Response mode: tree_summarize (synthesizes multiple conversations)\n",
      "Similarity top-k: 5 (gets 5 most relevant chunks)\n"
     ]
    }
   ],
   "source": [
    "## Create RAG Query Engine\n",
    "\n",
    "# Create a query engine with advanced settings\n",
    "query_engine = index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",  # Better for synthesizing multiple sources\n",
    "    similarity_top_k=5,              # Get top 5 most relevant chunks\n",
    "    verbose=True                     # Show what it's doing\n",
    ")\n",
    "\n",
    "print(\"✅ RAG Query Engine created\")\n",
    "print(\"Response mode: tree_summarize (synthesizes multiple conversations)\")\n",
    "print(\"Similarity top-k: 5 (gets 5 most relevant chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7026f7a2-2868-4c75-9946-abf7abe24c69",
   "metadata": {},
   "source": [
    "## Full RAG Test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a6c9579-b006-4a73-b680-a8e75e9a20ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG Query ===\n",
      "What have I learned about AI?\n",
      "\n",
      "--- Synthesized Answer ---\n",
      "You’ve learned that AI is a broad field encompassing machine learning, deep learning, natural language processing, and computer vision. You understand that it’s rooted in statistical concepts, programming (primarily Python), and the ability to build and evaluate models. You’ve grasped the core concepts of supervised and unsupervised learning, and the importance of understanding the ethical considerations surrounding AI development. You’ve also recognized the need for a combination of technical knowledge, practical experience, and a strong understanding of business strategy and product management principles.\n",
      "\n",
      "--- Top Sources ---\n",
      "1. ce_score=4.979 | Michael — ChatGPT - Pursuing Authentic Dreams\n",
      "   Are there any in-person courses at University of Pennsylvania I can take to learn about AI and business or maybe some kind of AI coding boot camp of some sort o...\n",
      "2. ce_score=4.928 | Michael — ChatGPT - New chat\n",
      "   As an aspiring ai product manager, give me a learning plan on topics to know about ai and machine learning...\n",
      "3. ce_score=4.504 | ChatGPT — ChatGPT - New chat\n",
      "   Certainly! Aspiring to become an AI Product Manager is an exciting journey that requires a multidisciplinary understanding of various topics. Here's a comprehen...\n",
      "4. ce_score=3.761 | ChatGPT — ChatGPT - Pivoting to AI PM\n",
      "   Pivoting to an AI product management role or working on AI concepts involves a mix of building technical knowledge, showcasing relevant skills, and networking i...\n",
      "5. ce_score=3.321 | Michael — ChatGPT - Ilama Index explanation\n",
      "   Okay got it, I’m trying to intuitively understand the learning process since I’ve had a lot of experience making recipes from YouTube and I wanted to translate ...\n",
      "\n",
      "(latency: 3.19s) ============================================================\n"
     ]
    }
   ],
   "source": [
    "## Full RAG with manual CrossEncoder rerank (no extra deps)\n",
    "from llama_index.core import PromptTemplate\n",
    "from sentence_transformers import CrossEncoder\n",
    "from time import perf_counter\n",
    "\n",
    "Settings.llm = Ollama(model=\"gemma3:1b\", request_timeout=60.0, temperature=0.1)\n",
    "\n",
    "qa_tmpl = PromptTemplate(\n",
    "    \"You are a concise, grounded assistant.\\n\"\n",
    "    \"Only use the provided context. If an answer is not supported by the context, say you don't know.\\n\"\n",
    "    \"Do not include any external links or sources; cite only the provided source titles.\\n\"\n",
    "    \"Return a short synthesis.\\n\\n\"\n",
    "    \"Context:\\n{context_str}\\n\\n\"\n",
    "    \"Question: {query_str}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "# Cross-encoder for reranking\n",
    "ce = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rag_query_with_rerank(q: str, retrieve_k=10, top_n=5):\n",
    "    retriever = index.as_retriever(similarity_top_k=retrieve_k)\n",
    "    nodes = retriever.retrieve(q)\n",
    "\n",
    "    # Cross-encode (query, passage) pairs\n",
    "    pairs = [(q, n.text or \"\") for n in nodes]\n",
    "    scores = ce.predict(pairs)\n",
    "\n",
    "    # Pick top_n by CE score\n",
    "    ranked = sorted(zip(nodes, scores), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_nodes = [n for n, _ in ranked]\n",
    "\n",
    "    # Build grounded context\n",
    "    ctx = \"\\n\\n\".join(n.text for n in top_nodes if n.text)\n",
    "\n",
    "    # Synthesize with LLM using the grounded prompt\n",
    "    prompt = qa_tmpl.format(context_str=ctx, query_str=q)\n",
    "    resp = Settings.llm.complete(prompt)\n",
    "\n",
    "    return resp.text, ranked\n",
    "\n",
    "# Test\n",
    "test_queries = [\n",
    "    \"What have I learned about AI?\",\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"\\n=== RAG Query ===\\n{q}\")\n",
    "    t0 = perf_counter()\n",
    "    answer, ranked = rag_query_with_rerank(q, retrieve_k=12, top_n=5)\n",
    "    dt = perf_counter() - t0\n",
    "\n",
    "    print(\"\\n--- Synthesized Answer ---\")\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\n--- Top Sources ---\")\n",
    "    for i, (n, ce_score) in enumerate(ranked, 1):\n",
    "        md = n.metadata or {}\n",
    "        title = md.get(\"source_name\", \"Unknown\")\n",
    "        author = md.get(\"author\", \"Unknown\")\n",
    "        snip = (n.text or \"\").replace(\"\\n\", \" \")[:160]\n",
    "        print(f\"{i}. ce_score={ce_score:.3f} | {author} — {title}\")\n",
    "        print(f\"   {snip}...\")\n",
    "\n",
    "    print(f\"\\n(latency: {dt:.2f}s) {'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
